"""
é—®ç­”å¼•æ“ - æ£€ç´¢ä¸é—®ç­”æ ¸å¿ƒé€»è¾‘
"""

from typing import List, Dict, Optional
from src.embedder import get_embedder
from src.vector_store import get_vector_store
from src.loader import load_md_files, get_file_stats
from src.splitter import split_documents
from src.ai_service import get_ai_service
from config import TOP_K


class QAEngine:
    """
    é—®ç­”å¼•æ“ç±»
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–é—®ç­”å¼•æ“
        """
        self.embedder = get_embedder()
        self.vector_store = get_vector_store()
        self.ai_service = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    def build_index(self, docs_dir: str, recreate: bool = True) -> Dict:
        """
        æ„å»ºç´¢å¼•
        
        Args:
            docs_dir: æ–‡æ¡£ç›®å½•è·¯å¾„
            recreate: æ˜¯å¦é‡æ–°åˆ›å»ºç´¢å¼•
            
        Returns:
            æ„å»ºç»“æœç»Ÿè®¡
        """
        # 1. åŠ è½½æ–‡æ¡£
        print(f"\nğŸ“‚ æ‰«æç›®å½•: {docs_dir}")
        documents = load_md_files(docs_dir)
        file_stats = get_file_stats(documents)
        print(f"   æ‰¾åˆ° {file_stats['total_files']} ä¸ª md æ–‡ä»¶, å…± {file_stats['total_chars']} å­—ç¬¦")
        
        if not documents:
            return {"success": False, "message": "æœªæ‰¾åˆ°ä»»ä½• md æ–‡ä»¶"}
        
        # 2. åˆ†å‰²æ–‡æ¡£
        print("\nâœ‚ï¸  åˆ†å‰²æ–‡æ¡£...")
        try:
            chunks = split_documents(documents)
            print(f"   ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        except Exception as e:
            print(f"   âŒ åˆ†å‰²æ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {"success": False, "message": f"åˆ†å‰²æ–‡æ¡£å¤±è´¥: {e}"}
        
        # 3. ç”Ÿæˆå‘é‡ï¼ˆåˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡ºï¼‰
        print("\nğŸ”¢ ç”Ÿæˆå‘é‡...")
        texts = [chunk["chunk_text"] for chunk in chunks]
        
        # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ 100 ä¸ªæ–‡æœ¬å—
        batch_size = 100
        all_vectors = []
        vector_dim = None
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            print(f"   å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ ({i+1}-{min(i+batch_size, len(texts))}/{len(texts)})")
            batch_vectors = self.embedder.encode(batch_texts, show_progress=False)
            all_vectors.extend(batch_vectors.tolist())
            
            if vector_dim is None:
                vector_dim = batch_vectors.shape[1]
        
        print(f"   å‘é‡ç»´åº¦: {vector_dim}, æ€»æ•°: {len(all_vectors)}")
        
        # 4. åˆ›å»ºé›†åˆ
        print("\nğŸ’¾ å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
        self.vector_store.create_collection(
            dimension=vector_dim,
            recreate=recreate
        )
        
        # 5. åˆ†æ‰¹æ’å…¥æ•°æ®ï¼ˆé¿å…ä¸€æ¬¡æ€§æ’å…¥è¿‡å¤šæ•°æ®ï¼‰
        total_inserted = 0
        for i in range(0, len(all_vectors), batch_size):
            batch_vectors = all_vectors[i:i + batch_size]
            batch_chunks = chunks[i:i + batch_size]
            
            # é‡æ–°åˆ†é… ID
            batch_chunks_with_id = []
            for j, chunk in enumerate(batch_chunks):
                batch_chunks_with_id.append({
                    **chunk,
                    "id": i + j
                })
            
            ids = self.vector_store.insert(batch_vectors, batch_chunks_with_id)
            total_inserted += len(ids)
            print(f"   å·²æ’å…¥ {total_inserted}/{len(all_vectors)} æ¡è®°å½•")
        
        print(f"âœ… ç´¢å¼•å»ºç«‹å®Œæˆï¼")
        
        return {
            "success": True,
            "total_files": file_stats["total_files"],
            "total_chunks": len(chunks),
            "vector_dimension": vector_dim
        }
    
    def query(self, question: str, top_k: int = TOP_K) -> List[Dict]:
        """
        æŸ¥è¯¢é—®ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # 1. å°†é—®é¢˜ç¼–ç ä¸ºå‘é‡
        query_vector = self.embedder.encode(question)[0].tolist()
        
        # 2. åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
        results = self.vector_store.search(query_vector, top_k)
        
        return results
    
    def get_stats(self) -> Dict:
        """
        è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        return self.vector_store.get_collection_stats()
    
    def ask_with_ai(
        self,
        question: str,
        top_k: int = TOP_K,
        base_url: Optional[str] = None,
        api_key: Optional[str] = None,
        model: Optional[str] = None
    ) -> Dict:
        """
        ä½¿ç”¨ AI åŸºäºçŸ¥è¯†åº“å›ç­”é—®é¢˜ï¼ˆRAGï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢ç»“æœæ•°é‡
            base_url: API åŸºç¡€ URLï¼ˆå¯é€‰ï¼‰
            api_key: API å¯†é’¥ï¼ˆå¯é€‰ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒ…å«ç­”æ¡ˆã€æ£€ç´¢ç»“æœå’Œå…ƒä¿¡æ¯çš„å­—å…¸
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        search_results = self.query(question, top_k)
        
        if not search_results:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£",
                "answer": None,
                "contexts": []
            }
        
        # 2. æå–ä¸Šä¸‹æ–‡æ–‡æœ¬
        contexts = [result["text"] for result in search_results]
        
        # 3. åˆå§‹åŒ–æˆ–æ›´æ–° AI æœåŠ¡
        if base_url or api_key or model:
            self.ai_service = get_ai_service(base_url, api_key, model)
        elif self.ai_service is None:
            try:
                self.ai_service = get_ai_service()
            except ValueError as e:
                return {
                    "success": False,
                    "error": str(e),
                    "answer": None,
                    "contexts": search_results
                }
        
        # 4. ä½¿ç”¨ AI ç”Ÿæˆç­”æ¡ˆ
        ai_result = self.ai_service.generate_answer(question, contexts)
        
        # 5. è¿”å›å®Œæ•´ç»“æœ
        return {
            **ai_result,
            "contexts": search_results,
            "context_count": len(search_results)
        }


# å…¨å±€å•ä¾‹
_qa_engine_instance = None


def get_qa_engine() -> QAEngine:
    """
    è·å–å…¨å±€ QAEngine å®ä¾‹
    
    Returns:
        QAEngine å®ä¾‹
    """
    global _qa_engine_instance
    if _qa_engine_instance is None:
        _qa_engine_instance = QAEngine()
    return _qa_engine_instance
