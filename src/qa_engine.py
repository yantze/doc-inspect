"""
é—®ç­”å¼•æ“ - æ£€ç´¢ä¸é—®ç­”æ ¸å¿ƒé€»è¾‘
"""

from typing import List, Dict, Optional
from src.embedder import get_embedder
from src.vector_store import get_vector_store
from src.loader import load_md_files, get_file_stats
from src.splitter import split_documents
from src.ai_service import get_ai_service
from config import TOP_K


class QAEngine:
    """
    é—®ç­”å¼•æ“ç±»
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–é—®ç­”å¼•æ“
        """
        self.embedder = get_embedder()
        self.vector_store = get_vector_store()
        self.ai_service = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    def build_index(self, docs_dir: str, recreate: bool = True) -> Dict:
        """
        æ„å»ºç´¢å¼•
        
        Args:
            docs_dir: æ–‡æ¡£ç›®å½•è·¯å¾„
            recreate: æ˜¯å¦é‡æ–°åˆ›å»ºç´¢å¼•
            
        Returns:
            æ„å»ºç»“æœç»Ÿè®¡
        """
        # 1. åŠ è½½æ–‡æ¡£
        print(f"\nğŸ“‚ æ‰«æç›®å½•: {docs_dir}")
        documents = load_md_files(docs_dir)
        file_stats = get_file_stats(documents)
        print(f"   æ‰¾åˆ° {file_stats['total_files']} ä¸ª md æ–‡ä»¶, å…± {file_stats['total_chars']} å­—ç¬¦")
        
        if not documents:
            return {"success": False, "message": "æœªæ‰¾åˆ°ä»»ä½• md æ–‡ä»¶"}
        
        # 2. åˆ†å‰²æ–‡æ¡£
        print("\nâœ‚ï¸  åˆ†å‰²æ–‡æ¡£...")
        chunks = split_documents(documents)
        print(f"   ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        # 3. ç”Ÿæˆå‘é‡
        print("\nğŸ”¢ ç”Ÿæˆå‘é‡...")
        texts = [chunk["chunk_text"] for chunk in chunks]
        vectors = self.embedder.encode(texts, show_progress=True)
        print(f"   å‘é‡ç»´åº¦: {vectors.shape[1]}")
        
        # 4. åˆ›å»ºé›†åˆå¹¶æ’å…¥æ•°æ®
        print("\nğŸ’¾ å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
        self.vector_store.create_collection(
            dimension=vectors.shape[1],
            recreate=recreate
        )
        
        # å°† numpy æ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨
        vectors_list = vectors.tolist()
        ids = self.vector_store.insert(vectors_list, chunks)
        print(f"   æ’å…¥ {len(ids)} æ¡è®°å½•")
        
        return {
            "success": True,
            "total_files": file_stats["total_files"],
            "total_chunks": len(chunks),
            "vector_dimension": vectors.shape[1]
        }
    
    def query(self, question: str, top_k: int = TOP_K) -> List[Dict]:
        """
        æŸ¥è¯¢é—®ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # 1. å°†é—®é¢˜ç¼–ç ä¸ºå‘é‡
        query_vector = self.embedder.encode(question)[0].tolist()
        
        # 2. åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
        results = self.vector_store.search(query_vector, top_k)
        
        return results
    
    def get_stats(self) -> Dict:
        """
        è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        return self.vector_store.get_collection_stats()
    
    def ask_with_ai(
        self,
        question: str,
        top_k: int = TOP_K,
        base_url: Optional[str] = None,
        api_key: Optional[str] = None,
        model: Optional[str] = None
    ) -> Dict:
        """
        ä½¿ç”¨ AI åŸºäºçŸ¥è¯†åº“å›ç­”é—®é¢˜ï¼ˆRAGï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢ç»“æœæ•°é‡
            base_url: API åŸºç¡€ URLï¼ˆå¯é€‰ï¼‰
            api_key: API å¯†é’¥ï¼ˆå¯é€‰ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒ…å«ç­”æ¡ˆã€æ£€ç´¢ç»“æœå’Œå…ƒä¿¡æ¯çš„å­—å…¸
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        search_results = self.query(question, top_k)
        
        if not search_results:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£",
                "answer": None,
                "contexts": []
            }
        
        # 2. æå–ä¸Šä¸‹æ–‡æ–‡æœ¬
        contexts = [result["text"] for result in search_results]
        
        # 3. åˆå§‹åŒ–æˆ–æ›´æ–° AI æœåŠ¡
        if base_url or api_key or model:
            self.ai_service = get_ai_service(base_url, api_key, model)
        elif self.ai_service is None:
            try:
                self.ai_service = get_ai_service()
            except ValueError as e:
                return {
                    "success": False,
                    "error": str(e),
                    "answer": None,
                    "contexts": search_results
                }
        
        # 4. ä½¿ç”¨ AI ç”Ÿæˆç­”æ¡ˆ
        ai_result = self.ai_service.generate_answer(question, contexts)
        
        # 5. è¿”å›å®Œæ•´ç»“æœ
        return {
            **ai_result,
            "contexts": search_results,
            "context_count": len(search_results)
        }


# å…¨å±€å•ä¾‹
_qa_engine_instance = None


def get_qa_engine() -> QAEngine:
    """
    è·å–å…¨å±€ QAEngine å®ä¾‹
    
    Returns:
        QAEngine å®ä¾‹
    """
    global _qa_engine_instance
    if _qa_engine_instance is None:
        _qa_engine_instance = QAEngine()
    return _qa_engine_instance
